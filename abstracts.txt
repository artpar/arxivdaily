Generative neural models have recently achieved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these generative models. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve accuracy while exploring significantly less of the search space. Applied to the model of Choe and Charniak (2016), our inference procedure obtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior state-of-the-art results for single-model systems.
Evaluating aesthetic value of digital photographs is a challenging task, mainly due to numerous factors that need to be taken into account and subjective manner of this process. In this paper, we propose to approach this problem using deep convolutional neural networks. Using a dataset of over 1.7 million photos collected from Flickr, we train and evaluate a deep learning model whose goal is to classify input images by analysing their aesthetic value. The result of this work is a publicly available Web-based application that can be used in several real-life applications, e.g. to improve the workflow of professional photographers by pre-selecting the best photos.
Visual Teach and Repeat (VT\&R) allows an autonomous vehicle to repeat a previously traversed route without a global positioning system. Existing implementations of VT\&R typically rely on 3D sensors such as stereo cameras for mapping and localization, but many mobile robots are equipped with only 2D monocular vision for tasks such as teleoperated bomb disposal. While simultaneous localization and mapping (SLAM) algorithms exist that can recover 3D structure and motion from monocular images, the scale ambiguity inherent in these methods complicates the estimation and control of lateral path-tracking error, which is essential for achieving high-accuracy path following. In this paper, we propose a monocular vision pipeline that enables kilometre-scale route repetition with centimetre-level accuracy by approximating the ground surface near the vehicle as planar (with some uncertainty) and recovering absolute scale from the known position and orientation of the camera relative to the vehicle. This system provides added value to many existing robots by allowing for high-accuracy autonomous route repetition with a simple software upgrade and no additional sensors. We validate our system over 4.3 km of autonomous navigation and demonstrate accuracy on par with the conventional stereo pipeline, even in highly non-planar terrain.
We present a method to match three dimensional shapes under non-isometric deformations, topology changes and partiality. We formulate the problem as matching between a set of pair-wise and point-wise descriptors, imposing a continuity prior on the mapping, and propose a projected descent optimization procedure inspired by difference of convex functions (DC) programming. Surprisingly, in spite of the highly non-convex nature of the resulting quadratic assignment problem, our method converges to a semantically meaningful and continuous mapping in most of our experiments, and scales well. We provide preliminary theoretical analysis and several interpretations of the method.
Opinion mining and sentiment analysis in social media is a research issue having a great interest in the scientific community. However, before begin this analysis, we are faced with a set of problems. In particular, the problem of the richness of languages and dialects within these media. To address this problem, we propose in this paper an approach of construction and implementation of Syntactic analyzer named ASDA. This tool represents a parser for the Algerian dialect that label the terms of a given corpus. Thus, we construct a labeling table containing for each term its stem, different prefixes and suffixes, allowing us to determine the different grammatical parts a sort of POS tagging. This labeling will serve us later in the semantic processing of the Algerian dialect, like the automatic translation of this dialect or sentiment analysis
Stringent mobile usage characteristics force wire- less networks to undergo a paradigm shift from conventional connection-centric to content-centric deployment. With respect to 5G, caching and heterogeneous networks (HetNet) are key technologies that will facilitate the evolution of highly content- centric networks by facilitating unified quality of service in terms of low-latency communication. In this paper, we study the impact of transceiver caching on the latency for a HetNet consisting of a single user, a receiver and one cache-assisted transceiver. We define an information-theoretic metric, the delivery time per bit (DTB), that captures the delivery latency. We establish coinciding lower and upper bounds on the DTB as a function of cache size and wireless channel parameters; thus, enabling a complete characterization of the DTB optimality of the network under study. As a result, we identify cache beneficial and non-beneficial channel regimes.
Rapid growth in the field of quantitative digital image analysis is paving the way for researchers to make precise measurements about objects in an image. To compute quantities from the image such as the density of compressed materials or the velocity of a shockwave, we must determine object boundaries. Images containing regions that each have a spatial trend in intensity are of particular interest. We present a supervised image segmentation method that incorporates spatial information to locate boundaries between regions with overlapping intensity histograms. The segmentation of a pixel is determined by comparing its intensity to distributions from local, nearby pixel intensities. Because of the statistical nature of the algorithm, we use maximum likelihood estimation theory to quantify uncertainty about each boundary. We demonstrate the success of this algorithm on a radiograph of a multicomponent cylinder and on an optical image of a laser-induced shockwave, and we provide final boundary locations with associated bands of uncertainty.
Smart optical networks are the next evolution of programmable networking and programmable automation of optical networks, with human-in-the-loop network control and management. The paper discusses this evolution and the role of Artificial Intelligence (AI).
Mutation testing has been widely used to assess the fault-detection effectiveness of a test suite, as well as to guide test case generation or prioritization. Empirical studies have shown that, while mutants are generally representative of real faults, an effective application of mutation testing requires "traditional" operators designed for programming languages to be augmented with operators specific to an application domain and/or technology. This paper proposes MDroid+, a framework for effective mutation testing of Android apps. First, we systematically devise a taxonomy of 262 types of Android faults grouped in 14 categories by manually analyzing 2,023 software artifacts from different sources (e.g., bug reports, commits). Then, we identified a set of 38 mutation operators, and implemented an infrastructure to automatically seed mutations in Android apps with 35 of the identified operators. The taxonomy and the proposed operators have been evaluated in terms of stillborn/trivial mutants generated and their capacity to represent real faults in Android apps, as compared to other well know mutation tools.
During the 125th European Study Group with Industry held in Limassol, Cyprus, 5-9 December 2016, one of the participating companies, Engino.net Ltd, posed a very interesting challenge to the members of the study group. Engino.net Ltd is a Cypriot company, founded in 2004, that produces a series of toy sets -- the Engino® toy sets -- consisting of a number of building blocks which can be assembled by pupils to compose toy models. Depending on the contents of a particular toy set, the company has developed a number of models that can be built utilizing the blocks present in the set, however the production of a step-by-step assembly manual for each model could only be done manually. The goal of the challenge posed by the company was to implement a procedure to automatically generate the assembly instructions for a given toy. In the present paper we propose a graph-theoretic approach to model the problem and provide a series of results to solve it by employing modified versions of well established algorithms in graph theory. An algorithmic procedure to obtain a hierarchical, physically feasible decomposition of a given toy model, from which the assembly instructions can be recovered, is proposed.
Remote attestation (RA) is a popular means of detecting malware in embedded and IoT devices. RA is usually realized as an interactive protocol, whereby a trusted party -- verifier -- measures integrity of a potentially compromised remote device -- prover. Early work focused on purely software-based and fully hardware-based techniques, neither of which is ideal for low-end devices. More recent results have yielded hybrid (SW/HW) security architectures comprised of a minimal set of features to support efficient and secure RA on low-end devices.
All prior RA techniques require on-demand operation, i.e, RA is performed in real time. We identify some drawbacks of this general approach in the context of unattended devices: First, it fails to detect mobile malware that enters and leaves the prover between successive RA instances. Second, it requires the prover to engage in a potentially expensive (in terms of time and energy) computation, which can be harmful for critical or real-time devices.
To address these drawbacks, we introduce the concept of self-measurement where a prover device periodically (and securely) measures and records its own software state, based on a pre-established schedule. A possibly untrusted verifier occasionally collects and verifies these measurements. We present the design of a concrete technique called ERASMUS : Efficient Remote Attestation via Self-Measurement for Unattended Settings, justify its features and evaluate its performance. In the process, we also define a new metric -- Quality of Attestation (QoA). We argue that ERASMUS is well-suited for time-sensitive and/or safety-critical applications that are not served well by on-demand RA. Finally, we show that ERASMUS is a promising stepping stone towards handling attestation of multiple devices (i.e., a group or swarm) with high mobility.
We introduce and describe the results of a novel shared task on bandit learning for machine translation. The task was organized jointly by Amazon and Heidelberg University for the first time at the Second Conference on Machine Translation (WMT 2017). The goal of the task is to encourage research on learning machine translation from weak user feedback instead of human references or post-edits. On each of a sequence of rounds, a machine translation system is required to propose a translation for an input, and receives a real-valued estimate of the quality of the proposed translation for learning. This paper describes the shared task's learning and evaluation setup, using services hosted on Amazon Web Services (AWS), the data and evaluation metrics, and the results of various machine translation architectures and learning protocols.
Robots performing manipulation tasks must operate under uncertainty about both their pose and the dynamics of the system. In order to remain robust to modeling error and shifts in payload dynamics, agents must simultaneously perform estimation and control tasks. However, the optimal estimation actions are often not the optimal actions for accomplishing the control tasks, and thus agents trade between exploration and exploitation. This work frames the problem as a Bayes-adaptive Markov decision process and solves it online using Monte Carlo tree search and an extended Kalman filter to handle Gaussian process noise and parameter uncertainty in a continuous space. MCTS selects control actions to reduce model uncertainty and reach the goal state nearly optimally. Certainty equivalent model predictive control is used as a benchmark to compare performance in simulations with varying process noise and parameter uncertainty.
The present paper deals with online convex optimization involving both time-varying loss functions, and time-varying constraints. The loss functions are not fully accessible to the learner, and instead only the function values (a.k.a. bandit feedback) are revealed at queried points. The constraints are revealed after making decisions, and can be instantaneously violated, yet they must be satisfied in the long term. This setting fits nicely the emerging online network tasks such as fog computing in the Internet-of-Things (IoT), where online decisions must flexibly adapt to the changing user preferences (loss functions), and the temporally unpredictable availability of resources (constraints). Tailored for such human-in-the-loop systems where the loss functions are hard to model, a family of bandit online saddle-point (BanSaP) schemes are developed, which adaptively adjust the online operations based on (possibly multiple) bandit feedback of the loss functions, and the changing environment. Performance here is assessed by: i) dynamic regret that generalizes the widely used static regret; and, ii) fit that captures the accumulated amount of constraint violations. Specifically, BanSaP is proved to simultaneously yield sub-linear dynamic regret and fit, provided that the best dynamic solutions vary slowly over time. Numerical tests in fog computation offloading tasks corroborate that our proposed BanSaP approach offers competitive performance relative to existing approaches that are based on gradient feedback.
Sample efficiency is important when optimizing parameters of locomotion controllers, since hardware experiments are time consuming and expensive. Bayesian Optimization, a sample-efficient optimization framework, has recently been widely applied to address this problem, but further improvements in sample efficiency are needed for practical applicability to real-world robots and high-dimensional controllers. To address this, prior work has proposed using domain expertise for constructing custom distance metrics for locomotion. In this work we show how to learn such a distance metric automatically, without relying on domain experts. We use a neural network to learn an informed distance metric from data obtained in high-fidelity simulations. We conduct experiments on two different controllers and robot architectures. First, we demonstrate improvement in sample efficiency when optimizing a 5-dimensional controller on the ATRIAS robot hardware. We then conduct simulation experiments to optimize a 16-dimensional controller for a 7-link robot model and obtain significant improvements even when optimizing in perturbed environments. This demonstrates that our approach is able to enhance sample efficiency for two different controllers, hence is a fitting candidate for further experiments on hardware in the future.
In a controlled experiment of sequence-to-sequence approaches for the task of sentence correction, we find that character-based models are generally more effective than word-based models and models that encode subword information via convolutions, and that modeling the output data as a series of diffs improves effectiveness over standard approaches. Our strongest sequence-to-sequence model improves over our strongest phrase-based statistical machine translation model, with access to the same data, by 6 M2 (0.5 GLEU) points. Additionally, in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches.
Tartan (TRT), a hardware accelerator for inference with Deep Neural Networks (DNNs), is presented and evaluated on Convolutional Neural Networks. TRT exploits the variable per layer precision requirements of DNNs to deliver execution time that is proportional to the precision p in bits used per layer for convolutional and fully-connected layers. Prior art has demonstrated an accelerator with the same execution performance only for convolutional layers. Experiments on image classification CNNs show that on average across all networks studied, TRT outperforms a state-of-the-art bit-parallel accelerator by 1:90x without any loss in accuracy while it is 1:17x more energy efficient. TRT requires no network retraining while it enables trading off accuracy for additional improvements in execution performance and energy efficiency. For example, if a 1% relative loss in accuracy is acceptable, TRT is on average 2:04x faster and 1:25x more energy efficient than a conventional bit-parallel accelerator. A Tartan configuration that processes 2-bits at time, requires less area than the 1-bit configuration, improves efficiency to 1:24x over the bit-parallel baseline while being 73% faster for convolutional layers and 60% faster for fully-connected layers is also presented.
In this paper, the downlink of a typical massive MIMO system is studied when each base station is composed of three antenna arrays with directional antenna elements serving 120 degrees of the two-dimensional space. A lower bound for the achievable rate is provided. Furthermore, a power optimization problem is formulated and as a result, centralized and decentralized power allocation schemes are proposed. The simulation results reveal that using directional antennas at base stations along with sectoring can lead to a notable increase in the achievable rates by increasing the received signal power and decreasing 'pilot contamination' interference in multicell massive MIMO systems. Moreover, it is shown that using optimized power allocation can increase 0.95-likely rate in the system significantly.
Discriminative clustering has been successfully applied to a number of weakly-supervised learning tasks. Such applications include person and action recognition, text-to-video alignment, object co-segmentation and colocalization in videos and images. One drawback of discriminative clustering, however, is its limited scalability. We address this issue and propose an online optimization algorithm based on the Block-Coordinate Frank-Wolfe algorithm. We apply the proposed method to the problem of weakly supervised learning of actions and actors from movies together with corresponding movie scripts. The scaling up of the learning problem to 66 feature length movies enables us to significantly improve weakly supervised action recognition.
We address the task of entity-relationship (E-R) retrieval, i.e, given a query characterizing types of two or more entities and relationships between them, retrieve the relevant tuples of related entities. Answering E-R queries requires gathering and joining evidence from multiple unstructured documents. In this work, we consider entity and relationships of any type, i.e, characterized by context terms instead of pre-defined types or relationships. We propose a novel IR-centric approach for E-R retrieval, that builds on the basic early fusion design pattern for object retrieval, to provide extensible entity-relationship representations, suitable for complex, multi-relationships queries. We performed experiments with Wikipedia articles as entity representations combined with relationships extracted from ClueWeb-09-B with FACC1 entity linking. We obtained promising results using 3 different query collections comprising 469 E-R queries.
Proving that a cryptographic protocol is correct for secrecy is a hard task. One of the strongest strategies to reach this goal is to show that it is increasing, which means that the security level of every single atomic message exchanged in the protocol, safely evaluated, never deceases. Recently, two families of functions have been proposed to measure the security level of atomic messages. The first one is the family of interpretation-functions. The second is the family of witness-functions. In this paper, we show that the witness-functions are more efficient than interpretation-functions. We give a detailed analysis of an ad-hoc protocol on which the witness-functions succeed in proving its correctness for secrecy while the interpretation-functions fail to do so.
In this article we study the transfer learning model of action advice under a budget. We focus on reinforcement learning teachers providing action advice to heterogeneous students playing the game of Pac-Man under a limited advice budget. First, we examine several critical factors affecting advice quality in this setting, such as the average performance of the teacher, its variance and the importance of reward discounting in advising. The experiments show the non-trivial importance of the coefficient of variation (CV) as a statistic for choosing policies that generate advice. The CV statistic relates variance to the corresponding mean. Second, the article studies policy learning for distributing advice under a budget. Whereas most methods in the relevant literature rely on heuristics for advice distribution we formulate the problem as a learning one and propose a novel RL algorithm capable of learning when to advise, adapting to the student and the task at hand. Furthermore, we argue that learning to advise under a budget is an instance of a more generic learning problem: Constrained Exploitation Reinforcement Learning.
Visual localization enables autonomous vehicles to navigate in their surroundings and Augmented Reality applications to link virtual to real worlds. In order to be practically relevant, visual localization approaches need to be robust to a wide variety of viewing condition, including day-night changes, as well as weather and seasonal variations. In this paper, we introduce the first benchmark datasets specifically designed for analyzing the impact of such factors on visual localization. Using carefully created ground truth poses for query images taken under a wide variety of conditions, we evaluate the impact of various factors on the quality of 6 degree-of-freedom (6DOF) camera pose estimation through extensive experiments with state-of-the-art localization approaches. Based on our results, we draw conclusions about the difficulty of different conditions and propose promising avenues for future work. We will eventually make our two novel benchmarks publicly available.
Massive multiple-input multiple-output (MIMO) systems need to support massive connectivity for the application of the Internet of things (IoT). The overhead of channel state information (CSI) acquisition becomes a bottleneck in the system performance due to the increasing number of users. An intermittent estimation scheme is proposed to ease the burden of channel estimation and maximize the sum capacity. In the scheme, we exploit the temporal correlation of MIMO channels and analyze the influence of the age of CSI on the downlink transmission rate using linear precoders. We show the CSI updating interval should follow a quasi-periodic distribution and reach a trade-off between the accuracy of CSI estimation and the overhead of CSI acquisition by optimizing the CSI updating frequency of each user. Numerical results show that the proposed intermittent scheme provides significant capacity gains over the conventional continuous estimation scheme.
Modelling of multivariate densities is a core component in many signal processing, pattern recognition and machine learning applications. The modelling is often done via Gaussian mixture models (GMMs), which use computationally expensive and potentially unstable training algorithms. We provide an overview of a fast and robust implementation of GMMs in the C++ language, employing multi-threaded versions of the Expectation Maximisation (EM) and k-means training algorithms. Multi-threading is achieved through reformulation of the EM and k-means algorithms into a MapReduce-like framework. Furthermore, the implementation uses several techniques to improve numerical stability and modelling accuracy. We demonstrate that the multi-threaded implementation achieves a speedup of an order of magnitude on a recent 16 core machine, and that it can achieve higher modelling accuracy than a previously well-established publically accessible implementation. The multi-threaded implementation is included as a user-friendly class in recent releases of the open source Armadillo C++ linear algebra library. The library is provided under the permissive Apache~2.0 license, allowing unencumbered use in commercial products.
Artificial General Intelligence (AGI) or Strong AI aims to create machines with human-like or human-level intelligence, which is still a very ambitious goal when compared to the existing computing and AI systems. After many hype cycles and lessons from AI history, it is clear that a big conceptual leap is needed for crossing the starting line to kick-start mainstream AGI research. This position paper aims to make a small conceptual contribution toward reaching that starting line. After a broad analysis of the AGI problem from different perspectives, a system-theoretic and engineering-based research approach is introduced, which builds upon the existing mainstream AI and systems foundations. Several promising cross-fertilization opportunities between systems disciplines and AI research are identified. Specific potential research directions are discussed.
This paper considers the beamspace channel estimation problem in 3D lens antenna array under a millimeter-wave communication system. We analyze the focusing capability of the 3D lens antenna array and the sparsity of the beamspace channel response matrix. Considering the analysis, we observe that the channel matrix can be treated as a 2D natural image; that is, the channel is sparse, and the changes between adjacent elements are subtle. Thus, for the channel estimation, we incorporate an image reconstruction technique called sparse non-informative parameter estimator-based cosparse analysis AMP for imaging (SCAMPI) algorithm. The SCAMPI algorithm is faster and more accurate than earlier algorithms such as orthogonal matching pursuit and support detection algorithms. To further improve the SCAMPI algorithm, we model the channel distribution as a generic Gaussian mixture (GM) probability and embed the expectation maximization learning algorithm into the SCAMPI algorithm to learn the parameters in the GM probability. We show that the GM probability outperforms the common uniform distribution used in image reconstruction. We also propose a phase-shifter-reduced selection network structure to decrease the power consumption of the system and prove that the SCAMPI algorithm is robust even if the number of phase shifters is reduced by 10%.
Machine comprehension(MC) style question answering is a representative problem in natural language processing. Previous methods rarely spend time on the improvement of encoding layer, especially the embedding of syntactic information and name entity of the words, which are very crucial to the quality of encoding. Moreover, existing attention methods represent each query word as a vector or use a single vector to represent the whole query sentence, neither of them can handle the proper weight of the key words in query sentence. In this paper, we introduce a novel neural network architecture called Multi-layer Embedding with Memory Network(MEMEN) for machine reading task. In the encoding layer, we employ classic skip-gram model to the syntactic and semantic information of the words to train a new kind of embedding layer. We also propose a memory network of full-orientation matching of the query and passage to catch more pivotal information. Experiments show that our model has competitive results both from the perspectives of precision and efficiency in Stanford Question Answering Dataset(SQuAD) among all published results and achieves the state-of-the-art results on TriviaQA dataset.
The Earth observation satellites have been monitoring the earth's surface for a long time, and the images taken by the satellites contain large amounts of valuable data. However, it is extremely hard work to manually analyze such huge data. Thus, a method of automatic object detection is needed for satellite images to facilitate efficient data analyses. This paper describes a new image feature extended from higher-order local autocorrelation to the object detection of multispectral satellite images. The feature has been extended to extract spectral inter-relationships in addition to spatial relationships to fully exploit multispectral information. The results of experiments with object detection tasks conducted to evaluate the effectiveness of the proposed feature extension indicate that the feature realized a higher performance compared to existing methods.
We present a new method for training pedestrian detectors on an unannotated image set, which is captured by a moving camera with a fixed height and angle from the ground. Our approach is general and robust and makes no other assumptions about the image dataset or the number of pedestrians. We automatically extract the vanishing point and the scale of the pedestrians to calibrate the virtual camera and generate a probability map for the pedestrians to spawn. Using these features, we overlay synthetic human-like agents in proper locations on the images from the unannotated dataset. We also present novel techniques to increase the realism of these synthetic agents and use the augmented images to train a Faster R-CNN detector. Our approach improves the accuracy by 12 to 13 percent over prior methods for unannotated image datasets.
When approaching a novel visual recognition problem in a specialized image domain, a common strategy is to start with a pre-trained deep neural network and fine-tune it to the specialized domain. If the target domain covers a smaller visual space than the source domain used for pre-training (e.g. ImageNet), the fine-tuned network is likely to be over-parameterized. However, applying network pruning as a post-processing step to reduce the memory requirements has drawbacks: fine-tuning and pruning are performed independently; pruning parameters are set once and cannot adapt over time; and the highly parameterized nature of state-of-the-art pruning methods make it prohibitive to manually search the pruning parameter space for deep networks, leading to coarse approximations. We propose a principled method for jointly fine-tuning and compressing a pre-trained convolutional network that overcomes these limitations. Experiments on two specialized image domains (remote sensing images and describable textures) demonstrate the validity of the proposed approach.
We study the ensemble performance of biometric authentication systems, based on secret key generation, which work as follows. In the enrollment stage, an individual provides a biometric signal that is mapped into a secret key and a helper message, the former being prepared to become available to the system at a later time (for authentication), and the latter is stored in a public database. When an authorized user requests authentication, claiming his/her identity as one of the subscribers, s/he has to provide a biometric signal again, and then the system, which retrieves also the helper message of the claimed subscriber, produces an estimate of the secret key, that is finally compared to the secret key of the claimed user. In case of a match, the authentication request is approved, otherwise, it is rejected.Referring to an ensemble of systems based on Slepian-Wolf binning, we provide a detailed analysis of the false-reject and false-accept probabilities, for a wide class of stochastic decoders. We also comment on the security for the typical code in the ensemble.
Developed in [Deng and Lin, 2014], Least-Squares Progressive Iterative Approximation (LSPIA) is an efficient iterative method for solving B-spline curve and surface least-squares fitting systems. In [Deng and Lin 2014], it was shown that LSPIA is convergent when the iterative matrix is nonsingular. In this paper, we will show that LSPIA is still convergent even the iterative matrix is singular.
Human beings tend to cooperate with close friends, therefore they have to construct strong social relationships to recieve cooperation from others. Therefore they should have acquired their strategies of social relationship construction through an evolutionary process. The behavior of social relationship construction is know as "social grooming." In this paper, we show that there are four classes including a human-like strategy in evolutionary dynamics of social grooming strategies based on an evolutionary game simulation. Social relationship strengths (as measured by frequency of social grooming) often show a much skewed distribution (a power law distribution). It may be due to time costs constraints on social grooming, because the costs are too large to ignore for having many strong social relationships. Evolution of humans' strategies of construction of social relationships may explain the origin of human intelligence based on a social brain hypothesis. We constructed an individual-based model to explore the evolutionary dynamics of social grooming strategies. The model is based on behavior to win over others by strengthening social relationships with cooperators. The results of evolutionary simulations show the four classes of evolutionary dynamics. The results depend on total resources and the ratio of each cooperator's resource to the number of cooperators. One of the four classes is similar to a human strategy, i.e. the strategies based on the Yule--Simon process of power law.
Matrix recovery is raised in many areas. In this paper, we build up a framework for almost everywhere matrix recovery which means to recover almost all the P∈⊂ℍp×q from Tr(AjP),j=1,…,N where Aj∈Vj⊂ℍp×q. We mainly focus on the following question: how many measurements are needed to recover almost all the matrices in ? For the case where both  and Vj are algebraic varieties, we use the tools from algebraic geometry to study the question and present some results to address it under many different settings.
Aiming at improving performance of visual classification in a cost-effective manner, this paper proposes an incremental semi-supervised learning paradigm called Deep Co-Space (DCS). Unlike many conventional semi-supervised learning methods usually performing within a fixed feature space, our DCS gradually propagates information from labeled samples to unlabeled ones along with deep feature learning. We regard deep feature learning as a series of steps pursuing feature transformation, i.e., projecting the samples from a previous space into a new one, which tends to select the reliable unlabeled samples with respect to this setting. Specifically, for each unlabeled image instance, we measure its reliability by calculating the category variations of feature transformation from two different neighborhood variation perspectives, and merged them into an unified sample mining criterion deriving from Hellinger distance. Then, those samples keeping stable correlation to their neighboring samples (i.e., having small category variation in distribution) across the successive feature space transformation, are automatically received labels and incorporated into the model for incrementally training in terms of classification. Our extensive experiments on standard image classification benchmarks (e.g., Caltech-256 and SUN-397) demonstrate that the proposed framework is capable of effectively mining from large-scale unlabeled images, which boosts image classification performance and achieves promising results compared to other semi-supervised learning methods.
How to establish the matching (or corresponding) between two different 3D shapes is a classical problem. This paper focused on the research on shape mapping of 3D mesh models, and proposed a shape mapping algorithm based on Hidden Markov Random Field and EM algorithm, as introducing a hidden state random variable associated with the adjacent blocks of shape matching when establishing HMRF. This algorithm provides a new theory and method to ensure the consistency of the edge data of adjacent blocks, and the experimental results show that the algorithm in this paper has a great improvement on the shape mapping of 3D mesh models.
To reap the benefits of dense small base station (SBS) deployment, innovative backhaul solutions are needed in order to manage scenarios in which high-speed ground backhaul links are either unavailable or limited in capacity. In this paper, a novel backhaul scheme that utilizes unmanned aerial vehicles (UAVs) as an on-demand flying network linking ground SBSs and the core network is proposed. The design of the aerial backhaul scheme is formulated as a network formation game among UAVs that seek to form a multi-hop backhaul network in the air. To solve this game, a myopic network formation algorithm which reaches a pairwise stable network upon convergence, is introduced. The proposed network formation algorithm enables the UAVs to form the necessary multi-hop backhaul network in a decentralized manner thus adapting the backhaul architecture to the dynamics of the network. Simulation results show that the proposed network formation algorithm achieves substantial performance gains in terms of both rate and delay reaching, respectively, up to 40% and 41% compared to the formation of direct communication links with the gateway node (for a network with 15 UAVs).
In this work, we explore an innovative strategy for image denoising by using convolutional neural networks (CNN) to learn pixel-distribution from noisy data. By increasing CNN's width with large reception fields and more channels in each layer, CNNs can reveal the ability to learn pixel-distribution, which is a prior existing in many different types of noise. The key to our approach is a discovery that wider CNNs tends to learn the pixel-distribution features, which provides the probability of that inference-mapping primarily relies on the priors instead of deeper CNNs with more stacked nonlinear layers. We evaluate our work: Wide inference Networks (WIN) on additive white Gaussian noise (AWGN) and demonstrate that by learning the pixel-distribution in images, WIN-based network consistently achieves significantly better performance than current state-of-the-art deep CNN-based methods in both quantitative and visual evaluations. \textit{Code and models are available at \url{https://github.com/cswin/WIN}}.
The goal of this paper is to determine the spatio-temporal location of actions in video. Where training from hard to obtain box annotations is the norm, we propose an intuitive and effective algorithm that localizes actions from their class label only. We are inspired by recent work showing that unsupervised action proposals selected with human point-supervision perform as well as using expensive box annotations. Rather than asking users to provide point supervision, we propose fully automatic visual cues that replace manual point annotations. We call the cues pseudo-annotations, introduce five of them, and propose a correlation metric for automatically selecting and combining them. Thorough evaluation on challenging action localization datasets shows that we reach results comparable to results with full box supervision. We also show that pseudo-annotations can be leveraged during testing to improve weakly- and strongly-supervised localizers.
We aim for zero-shot localization and classification of human actions in video. Where traditional approaches rely on global attribute or object classification scores for their zero-shot knowledge transfer, our main contribution is a spatial-aware object embedding. To arrive at spatial awareness, we build our embedding on top of freely available actor and object detectors. Relevance of objects is determined in a word embedding space and further enforced with estimated spatial preferences. Besides local object awareness, we also embed global object awareness into our embedding to maximize actor and object interaction. Finally, we exploit the object positions and sizes in the spatial-aware embedding to demonstrate a new spatio-temporal action retrieval scenario with composite queries. Action localization and classification experiments on four contemporary action video datasets support our proposal. Apart from state-of-the-art results in the zero-shot localization and classification settings, our spatial-aware embedding is even competitive with recent supervised action localization alternatives.
We consider the minimization of submodular functions subject to ordering constraints. We show that this optimization problem can be cast as a convex optimization problem on a space of uni-dimensional measures, with ordering constraints corresponding to first-order stochastic dominance. We propose new discretization schemes that lead to simple and efficient algorithms based on zero-th, first, or higher order oracles; these algorithms also lead to improvements without isotonic constraints. Finally, our experiments show that non-convex loss functions can be much more robust to outliers for isotonic regression, while still leading to an efficient optimization problem.
The problem of estimating a high-dimensional sparse vector θ∈ℝn from an observation in i.i.d. Gaussian noise is considered. The performance is measured using squared-error loss. An empirical Bayes shrinkage estimator, derived using a Bernoulli-Gaussian prior, is analyzed and compared with the well-known soft-thresholding estimator. We obtain concentration inequalities for the Stein's unbiased risk estimate and the loss function of both estimators. The results show that for large n, both the risk estimate and the loss function concentrate on deterministic values close to the true risk.
Depending on the underlying θ, either the proposed empirical Bayes (eBayes) estimator or soft-thresholding may have smaller loss. We consider a hybrid estimator that attempts to pick the better of the soft-thresholding estimator and the eBayes estimator by comparing their risk estimates. It is shown that: i) the loss of the hybrid estimator concentrates on the minimum of the losses of the two competing estimators, and ii) the risk of the hybrid estimator is within order 1n√ of the minimum of the two risks. Simulation results are provided to support the theoretical results. Finally, we use the eBayes and hybrid estimators as denoisers in the approximate message passing (AMP) algorithm for compressed sensing, and show that their performance is superior to the soft-thresholding denoiser in a wide range of settings.
The quantum enhanced classical sensor network consists of K clusters of Ne entangled quantum states that have been trialled r times, each feeding into a classical estimation process. Previous literature has shown that each cluster can {ideally} achieve an estimation variance of 1/N2er for sufficient r. We begin by deriving the optimal values for the minimum mean squared error of this quantum enhanced classical system. We then show that if noise is \emph{absent} in the classical estimation process, the mean estimation error will decay like Ω(1/KN2er). However, when noise is \emph{present} we find that the mean estimation error will decay like Ω(1/K), so that \emph{all} the sensing gains obtained from the individual quantum clusters will be lost.
The charge prediction task is to determine appropriate charges for a given case, which is helpful for legal assistant systems where the user input is fact description. We argue that relevant law articles play an important role in this task, and therefore propose an attention-based neural network method to jointly model the charge prediction task and the relevant article extraction task in a unified framework. The experimental results show that, besides providing legal basis, the relevant articles can also clearly improve the charge prediction results, and our full model can effectively predict appropriate charges for cases with different expression styles.
Person re-identification is best known as the problem of associating a single person that is observed from one or more disjoint cameras. The existing literature has mainly addressed such an issue, neglecting the fact that people usually move in groups, like in crowded scenarios. We believe that the additional information carried by neighboring individuals provides a relevant visual context that can be exploited to obtain a more robust match of single persons within the group. Despite this, re-identifying groups of people compound the common single person re-identification problems by introducing changes in the relative position of persons within the group and severe self-occlusions. In this paper, we propose a solution for group re-identification that grounds on transferring knowledge from single person re-identification to group re-identification by exploiting sparse dictionary learning. First, a dictionary of sparse atoms is learned using patches extracted from single person images. Then, the learned dictionary is exploited to obtain a sparsity-driven residual group representation, which is finally matched to perform the re-identification. Extensive experiments on the i-LIDS groups and two newly collected datasets show that the proposed solution outperforms state-of-the-art approaches.
The key challenge in multiagent learning is learning a best response to the behaviour of other agents, which may be non-stationary: if the other agents adapt their strategy as well, the learning target moves. Disparate streams of research have approached non-stationarity from several angles, which make a variety of implicit assumptions that make it hard to keep an overview of the state of the art and to validate the innovation and significance of new works. This survey presents a coherent overview of work that addresses opponent-induced non-stationarity with tools from game theory, reinforcement learning and multi-armed bandits. Further, we reflect on the principle approaches how algorithms model and cope with this non-stationarity, arriving at a new framework and five categories (in increasing order of sophistication): ignore, forget, respond to target models, learn models, and theory of mind. A wide range of state-of-the-art algorithms is classified into a taxonomy, using these categories and key characteristics of the environment (e.g., observability) and adaptation behaviour of the opponents (e.g., smooth, abrupt). To clarify even further we present illustrative variations of one domain, contrasting the strengths and limitations of each category. Finally, we discuss in which environments the different approaches yield most merit, and point to promising avenues of future research.
A novel data-driven nested stochastic robust optimization (DDNSRO) framework is proposed to systematically and automatically handle labeled multi-class uncertainty data in optimization problems. Uncertainty realizations in large datasets are often collected from various conditions, which are encoded by class labels. A group of Dirichlet process mixture models is employed for uncertainty modeling from the multi-class uncertainty data. The proposed data-driven nonparametric uncertainty model could automatically adjust its complexity based on the data structure and complexity, thus accurately capturing the uncertainty information. A DDNSRO framework is further proposed based on the data-driven uncertainty model through a bi-level optimization structure. The outer optimization problem follows a two-stage stochastic programming approach to optimize the expected objective across different classes of data; robust optimization is nested as the inner problem to ensure the robustness of the solution while maintaining computational tractability. A tailored column-and-constraint generation algorithm is further developed to solve the resulting multi-level optimization problem efficiently. Case studies on strategic planning of process networks are presented to demonstrate the applicability of the proposed framework.
Web archives are typically very broad in scope and extremely large in scale. This makes data analysis appear daunting, especially for non-computer scientists. These collections constitute an increasingly important source for researchers in the social sciences, the historical sciences and journalists interested in studying past events. However, there are currently no access methods that help users to efficiently access information, in particular about specific events, beyond the retrieval of individual disconnected documents. Therefore we propose a novel method to extract event-centric document collections from large scale Web archives. This method relies on a specialized focused extraction algorithm. Our experiments on the German Web archive (covering a time period of 19 years) demonstrate that our method enables the extraction of event-centric collections for different event types.
We propose a recurrent extension of the Ladder network, which is motivated by the inference required in hierarchical latent variable models. We demonstrate that the recurrent Ladder is able to handle a wide variety of complex learning tasks that benefit from iterative inference and temporal modeling. The architecture shows close-to-optimal results on temporal modeling of video data, competitive results on music modeling, and improved perceptual grouping based on higher order abstractions, such as stochastic textures and motion cues. We present results for fully supervised, semi-supervised, and unsupervised tasks. The results suggest that the proposed architecture and principles are powerful tools for learning a hierarchy of abstractions, handling temporal information, modeling relations and interactions between objects.
Future applications involving humanoid robots may require physical interaction between the robot and a dynamic environment. In this case, classical balancing and walking controllers that neglect the environment dynamics may not be sufficient for achieving a stable robot behaviour. This paper presents a modeling and control framework for balancing humanoid robots in contact with a dynamic environment. We first model the dynamics of the robot and the environment, together with the contact constraints. Then, a control strategy for stabilizing the extended system is proposed. Theoretical results are verified in simulation with a model of the robot iCub balancing on a seesaw.
Adding manually annotated prosodic information, specifically pitch accents and phrasing, to the typical text-based feature set for coreference resolution has previously been shown to have a positive effect on German data. Practical applications on spoken language, however, would rely on automatically predicted prosodic information. In this paper we predict pitch accents (and phrase boundaries) using a convolutional neural network (CNN) model from acoustic features extracted from the speech signal. After an assessment of the quality of these automatic prosodic annotations, we show that they also significantly improve coreference resolution.
Active Shape Models (ASM) are an iterative segmentation technique to find a landmark-based contour of an object. In each iteration, a least-squares fit of a plausible shape to some detected target landmarks is determined. Finding these targets is a critical step: some landmarks are more reliably detected than others, and some landmarks may not be within the field of view of their detectors. To add robustness while preserving simplicity at the same time, a generalized least-squares approach can be used, where a weighting matrix incorporates reliability information about the landmarks. We propose a strategy to choose this matrix, based on the covariance of empirically determined residuals of the fit. We perform a further step to determine whether the target landmarks are within the range of their detectors. We evaluate our strategy on fluoroscopic X-ray images to segment the femur. We show that our technique outperforms the standard ASM as well as other more heuristic weighted least-squares strategies.
Human pose forecasting is an important problem in computer vision with applications to human-robot interaction, visual surveillance, and autonomous driving. Usually, forecasting algorithms use 3D skeleton sequences and are trained to forecast for a few milliseconds into the future. Long-range forecasting is challenging due to the difficulty of estimating how long a person continues an activity. To this end, our contributions are threefold: (i) we propose a generative framework for poses using variational autoencoders based on Deep Markov Models (DMMs); (ii) we evaluate our pose forecasts using a pose-based action classifier, which we argue better reflects the subjective quality of pose forecasts than distance in coordinate space; (iii) last, for evaluation of the new model, we introduce a 480,000-frame video dataset called Ikea Furniture Assembly (Ikea FA), which depicts humans repeatedly assembling and disassembling furniture. We demonstrate promising results for our approach on both Ikea FA and the existing NTU RGB+D dataset.
Modern distributed systems often achieve availability and scalability by providing consistency guarantees about the data they manage weaker than linearizability. We consider a class of such consistency models that, despite this weakening, guarantee that clients eventually agree on a global sequence of operations, while seeing a subsequence of this final sequence at any given point of time. Examples of such models include the classical Total Store Order (TSO) and recently proposed dual TSO, Global Sequence Protocol (GSP) and Ordered Sequential Consistency.
We define a unified model, called Global Sequence Consistency (GSC), that has the above models as its special cases, and investigate its key properties. First, we propose a condition under which multiple objects each satisfying GSC can be composed so that the whole set of objects satisfies GSC. Second, we prove an interesting relationship between special cases of GSC---GSP, TSO and dual TSO: we show that clients that do not communicate out-of-band cannot tell the difference between these models. To obtain these results, we propose a novel axiomatic specification of GSC and prove its equivalence to the operational definition of the model.
When smartphones, applications (a.k.a, apps), and app stores have been widely adopted by the billions, an interesting debate emerges: whether and to what extent do device models influence the behaviors of their users? The answer to this question is critical to almost every stakeholder in the smartphone app ecosystem, including app store operators, developers, end-users, and network providers. To approach this question, we collect a longitudinal data set of app usage through a leading Android app store in China, called Wandoujia. The data set covers the detailed behavioral profiles of 0.7 million (761,262) unique users who use 500 popular types of Android devices and about 0.2 million (228,144) apps, including their app management activities, daily network access time, and network traffic of apps. We present a comprehensive study on investigating how the choices of device models affect user behaviors such as the adoption of app stores, app selection and abandonment, data plan usage, online time length, the tendency to use paid/free apps, and the preferences to choosing competing apps. Some significant correlations between device models and app usage are derived, leading to important findings on the various user behaviors. For example, users owning different device models have a substantial diversity of selecting competing apps, and users owning lower-end devices spend more money to purchase apps and spend more time under cellular network.
Recommender systems are personalized information systems. However, in many settings, the end-user of the recommendations is not the only party whose needs must be represented in recommendation generation. Incorporating this insight gives rise to the notion of multistakeholder recommendation, in which the interests of multiple parties are represented in recommendation algorithms and evaluation. In this paper, we identify patterns of stakeholder utility that characterize different multistakeholder recommendation applications, and provide a taxonomy of the different possible systems, only some of which have currently been implemented.
This paper proposes an arrayed-waveguide grating (AWG) based wavelength-division-multiplexing (WDM) shuffle network. Compared with previous optical shuffle networks, our proposal is compact, easy to implement, highly scalable, and cost effective.
Unfortunately, the article "A Comparative Study to Benchmark Cross-project Defect Prediction Approaches" has a problem in the statistical analysis which was pointed out almost immediately after the pre-print of the article appeared online. While the problem does not negate the contribution of the the article and all key findings remain the same, it does alter some rankings of approaches used in the study. Within this correction, we will explain the problem, how we resolved it, and present the updated results.
People detection methods are highly sensitive to the perpetual occlusions among the targets. As multi-camera set-ups become more frequently encountered, joint exploitation of the across views information would allow for improved detection performances. We provide a large-scale HD dataset named WILDTRACK which finally makes advanced deep learning methods applicable to this problem. The seven-static-camera set-up captures realistic and challenging scenarios of walking people.
Notably, its camera calibration with jointly high-precision projection widens the range of algorithms which may make use of this dataset. In aim to help accelerate the research on automatic camera calibration, such annotations also accompany this dataset.
Furthermore, the rich-in-appearance visual context of the pedestrian class makes this dataset attractive for monocular pedestrian detection as well, since: the HD cameras are placed relatively close to the people, and the size of the dataset further increases seven-fold.
In summary, we overview existing multi-camera datasets and detection methods, enumerate details of our dataset, and we benchmark multi-camera state of the art detectors on this new dataset.
In this paper, we provide an analysis of self-organized network management, with an end-to-end perspective of the network. Self-organization as applied to cellular networks is usually referred to Self-organizing Networks (SONs), and it is a key driver for improving Operations, Administration, and Maintenance (OAM) activities. SON aims at reducing the cost of installation and management of 4G and future 5G networks, by simplifying operational tasks through the capability to configure, optimize and heal itself. This autonomous management vision has to be extended to the end to end network, to satisfy 5G network management requirements. In literature and also in some instances of products available in the market, Machine Learning (ML) has been identified as the key tool to implement autonomous adaptability and take advantage of experience when making decisions. In this paper we survey how network management can significantly benefit from ML solutions. We review and provide the basic concepts and taxonomy for SON, network management and ML. We analyse the available state of the art in the literature, standardization and in the market. We pay special attention to 3GPP evolution in the area of network management and to the data that can be extracted from 3GPP networks, in order to gain knowledge and experience in how the network is working, and improve network performance in a proactive way. Finally, we go through the main challenges associated with this research line, in both 4G and in what 5G is getting designed, while identifying new directions for research.
In recent years, the networks of low-power devices have gained popularity. Typically these devices are wireless and interact to form large networks such as the Machine to Machine (M2M) networks, Internet of Things (IoT), Wearable Computing, and Wireless Sensor Networks. The collaboration among these devices is a key to achieving the full potential of these networks. A major problem in this field is to guarantee robust communication between elements while keeping the whole network energy efficient. In this paper, we introduce an extended and improved emergent broadcast slot (EBS) scheme, which facilitates collaboration for robust communication and is energy efficient. In the EBS, nodes communication unit remains in sleeping mode and are awake just to communicate. The EBS scheme is fully decentralized, that is, nodes coordinate their wake-up window in partially overlapped manner within each duty-cycle to avoid message collisions. We show the theoretical convergence behavior of the scheme, which is confirmed through real test-bed experimentation.
Nonnegative matrix factorization is a powerful technique to realize dimension reduction and pattern recognition through single-layer data representation learning. Deep learning, however, with its carefully designed hierarchical structure, is able to combine hidden features to form more representative features for pattern recognition. In this paper, we proposed sparse deep nonnegative matrix factorization models to analyze complex data for more accurate classification and better feature interpretation. Such models are designed to learn localized features or generate more discriminative representations for samples in distinct classes by imposing L1-norm penalty on the columns of certain factors. By extending one-layer model into multi-layer one with sparsity, we provided a hierarchical way to analyze big data and extract hidden features intuitively due to nonnegativity. We adopted the Nesterov's accelerated gradient algorithm to accelerate the computing process with the convergence rate of O(1/k2) after k steps iteration. We also analyzed the computing complexity of our framework to demonstrate their efficiency. To improve the performance of dealing with linearly inseparable data, we also considered to incorporate popular nonlinear functions into this framework and explored their performance. We applied our models onto two benchmarking image datasets, demonstrating our models can achieve competitive or better classification performance and produce intuitive interpretations compared with the typical NMF and competing multi-layer models.
Recent technology advancements in the areas of compute, storage and networking, along with the increased demand for organizations to cut costs while remaining responsive to increasing service demands have led to the growth in the adoption of cloud computing services. Cloud services provide the promise of improved agility, resiliency, scalability and a lowered Total Cost of Ownership (TCO). This research introduces a framework for minimizing cost and maximizing resource utilization by using an Integer Linear Programming (ILP) approach to optimize the assignment of workloads to servers on Amazon Web Services (AWS) cloud infrastructure. The model is based on the classical minimum-cost flow model, known as the assignment model.
Because of vast volume of data being produced by today's scientific simulations and experiments, lossy data compressor allowing user-controlled loss of accuracy during the compression is a relevant solution for significantly reducing the data size. However, lossy compressor developers and users are missing a tool to explore the features of scientific datasets and understand the data alteration after compression in a systematic and reliable way. To address this gap, we have designed and implemented a generic framework called Z-checker. On the one hand, Z-checker combines a battery of data analysis components for data compression. On the other hand, Z-checker is implemented as an open-source community tool to which users and developers can contribute and add new analysis components based on their additional analysis demands. In this paper, we present a survey of existing lossy compressors. Then we describe the design framework of Z-checker, in which we integrated evaluation metrics proposed in prior work as well as other analysis tools. Specifically, for lossy compressor developers, Z-checker can be used to characterize critical properties of any dataset to improve compression strategies. For lossy compression users, Z-checker can detect the compression quality, provide various global distortion analysis comparing the original data with the decompressed data and statistical analysis of the compression error. Z-checker can perform the analysis with either coarse granularity or fine granularity, such that the users and developers can select the best-fit, adaptive compressors for different parts of the dataset. Z-checker features a visualization interface displaying all analysis results in addition to some basic views of the datasets such as time series. To the best of our knowledge, Z-checker is the first tool designed to assess lossy compression comprehensively for scientific datasets.
While cluster computing frameworks are continuously evolving to provide real-time data analysis capabilities, Apache Spark has managed to be at the forefront of big data analytics for being a unified framework for both, batch and stream data processing. There is also a renewed interest is Near Data Computing (NDC) due to technological advancement in the last decade. However, it is not known if NDC architectures can improve the performance of big data processing frameworks such as Apache Spark. In this position paper, we hypothesize in favour of NDC architecture comprising programmable logic based hybrid 2D integrated processing-in-memory and in-storage processing for Apache Spark, by extensive profiling of Apache Spark based workloads on Ivy Bridge Server.
In dialogical argumentation it is often assumed that the involved parties always correctly identify the intended statements posited by each other, realize all of the associated relations, conform to the three acceptability states (accepted, rejected, undecided), adjust their views when new and correct information comes in, and that a framework handling only attack relations is sufficient to represent their opinions. Although it is natural to make these assumptions as a starting point for further research, removing them or even acknowledging that such removal should happen is more challenging for some of these concepts than for others. Probabilistic argumentation is one of the approaches that can be harnessed for more accurate user modelling. The epistemic approach allows us to represent how much a given argument is believed by a given person, offering us the possibility to express more than just three agreement states. It is equipped with a wide range of postulates, including those that do not make any restrictions concerning how initial arguments should be viewed, thus potentially being more adequate for handling beliefs of the people that have not fully disclosed their opinions in comparison to Dung's semantics. The constellation approach can be used to represent the views of different people concerning the structure of the framework we are dealing with, including cases in which not all relations are acknowledged or when they are seen differently than intended. Finally, bipolar argumentation frameworks can be used to express both positive and negative relations between arguments. In this paper we describe the results of an experiment in which participants judged dialogues in terms of agreement and structure. We compare our findings with the aforementioned assumptions as well as with the constellation and epistemic approaches to probabilistic argumentation and bipolar argumentation.
In the nineties Immerman and Medina initiated the search for syn- tactic tools to prove NP-completeness. In their work, amongst several results, they conjecture that the NP-completeness of a problem defined by the conjunction of a sentence in Existential Second Order Logic with a First Order sentence, necessarily imply the NP-completeness of the problem defined by the Existential Second Order sentence alone. This is interesting because if true it would justify the restriction heuristic pro- posed in Garey and Johnson in his classical book on NP completeness, which roughly says that in some cases one can prove NP- complete a problem A by proving NP-complete a problem B contained in A. Borges and Bonet extend some results from Immerman and Medina and they also prove for a host of complexity classes that the Immerman- Medina conjecture is true when the First Order sentence in the conjunc- tion is universal. Our work extends that result to the Second Level of the Polynomial-Time Hierarchy.
This paper addresses an open problem in traffic modeling: the second-order macroscopic node problem. A second-order macroscopic traffic model, in contrast to a first-order model, allows for variation of driving behavior across subpopulations of vehicles in the flow. The second-order models are thus more descriptive (e.g., they have been used to model variable mixtures of behaviorally-different traffic, like car/truck traffic, autonomous/human-driven traffic, etc.), but are much more complex. The second-order node problem is a particularly complex problem, as it requires the resolution of discontinuities in traffic density and mixture characteristics, and solving of throughflows for arbitrary numbers of input and output roads to a node (in other words, this is an arbitrary-dimensional Riemann problem with two conserved quantities). We propose a solution to this problem by making use of a recently-introduced dynamic system characterization of the first-order node model problem, which gives insight and intuition as to the continuous-time dynamics implicit in first-order node models. We use this intuition to extend the dynamic system node model to the second-order setting. We also extend the well-known "Generic Class of Node Model" constraints to the second order and present a simple solution algorithm to the second-order node problem. This node model has immediate applications in allowing modeling of behaviorally-complex traffic flows of contemporary interest (like partially-autonomous-vehicle flows) in arbitrary road networks.
Let k be a fixed integer. We determine the complexity of finding a p-partition (V1,…,Vp) of the vertex set of a given digraph such that the maximum out-degree of each of the digraphs induced by Vi, (1≤i≤p) is at least k smaller than the maximum out-degree of D. We show that this problem is polynomial-time solvable when p≥2k and -complete otherwise. The result for k=1 and p=2 answers a question posed in \cite{bangTCS636}. We also determine, for all fixed non-negative integers k1,k2,p, the complexity of deciding whether a given digraph of maximum out-degree p has a 2-partition (V1,V2) such that the digraph induced by Vi has maximum out-degree at most ki for i∈[2]. It follows from this characterization that the problem of deciding whether a digraph has a 2-partition (V1,V2) such that each vertex v∈Vi has at least as many neighbours in the set V3−i as in Vi, for i=1,2 is -complete. This solves a problem from \cite{kreutzerEJC24} on majority colourings.
Graphs provide a natural mathematical abstraction for systems with pairwise interactions, and thus have become a prevalent tool for the representation of systems across various scientific domains. However, as the size of relational datasets continues to grow, traditional graph-based approaches are increasingly replaced by other modeling paradigms, which enable a more flexible treatment of such datasets. A promising framework in this context is provided by graphons, which have been formally introduced as the natural limiting objects for graphs of increasing sizes. However, while the theory of graphons is already well developed, some prominent tools in network analysis still have no counterpart within the realm of graphons. In particular, node centrality measures, which have been successfully employed in various applications to reveal important nodes in a network, have so far not been defined for graphons. In this work we introduce formal definitions of centrality measures for graphons and establish their connections to centrality measures defined on finite graphs. In particular, we build on the theory of linear integral operators to define degree, eigenvector, and Katz centrality functions for graphons. We further establish concentration inequalities showing that these centrality functions are natural limits of their analogous counterparts defined on sequences of random graphs of increasing size. We discuss several strategies for computing these centrality measures, and illustrate them through a set of numerical examples.
Here we present a new approach to search for first order invariants (first integrals) of rational second order ordinary differential equations. This method is an alternative to the Darbouxian and symmetry approaches. Our procedure can succeed in many cases where these two approaches fail. We also present here a Maple implementation of the theoretical results and methods, hereby introduced, in a computational package -- {\it InSyDE}. The package is designed, apart from materializing the algorithms presented, to provide a set of tools to allow the user to analyse the intermediary steps of the process.
The goal of counterfactual learning for statistical machine translation (SMT) is to optimize a target SMT system from logged data that consist of user feedback to translations that were predicted by another, historic SMT system. A challenge arises by the fact that risk-averse commercial SMT systems deterministically log the most probable translation. The lack of sufficient exploration of the SMT output space seemingly contradicts the theoretical requirements for counterfactual learning. We show that counterfactual learning from deterministic bandit logs is possible nevertheless by smoothing out deterministic components in learning. This can be achieved by additive and multiplicative control variates that avoid degenerate behavior in empirical risk minimization. Our simulation experiments show improvements of up to 2 BLEU points by counterfactual learning from deterministic bandit feedback.
Approximate deconvolution forms a mathematical framework for the structural modeling of turbulence. The sub-filter scale flow quantities are typically recovered by using the Van Cittert iterative procedure. In this paper, however, we put forth a generalized approach for the iterative deconvolution process of sub-filter scale recovery of turbulent flows by introducing Krylov space iterative methods. Their accuracy and efficiency are demonstrated through a systematic a-priori analysis of solving the Kraichnan and Kolmogorov homogeneous isotropic turbulence problems in two- and three-dimensional domains, respectively. Our numerical assessments show that the conjugate gradient based iterative techniques lead to significantly improved performance over the Van Cittert procedure and offer great promise for approximate deconvolution turbulence models. In fact, our energy spectra analysis illustrates that a substantially longer inertial range can be recovered by using the proposed procedure equipped with the BiCGSTAB iterative scheme. This trend is also confirmed by capturing tails of the probability density function of turbulent flow quantities.
In this paper we address the problem of electing a committee among a set of m candidates and on the basis of the preferences of a set of n voters. We consider the approval voting method in which each voter can approve as many candidates as she/he likes by expressing a preference profile (boolean m-vector). In order to elect a committee, a voting rule must be established to `transform' the n voters' profiles into a winning committee. The problem is widely studied in voting theory; for a variety of voting rules the problem was shown to be computationally difficult and approximation algorithms and heuristic techniques were proposed in the literature. In this paper we follow an Ordered Weighted Averaging approach and study the k-sum approval voting (optimization) problem in the general case 1≤k<n. For this problem we provide different mathematical programming formulations that allow us to solve it in an exact solution framework. We provide computational results showing that our approach is efficient for medium-size test problems (n up to 200, m up to 60) since in all tested cases it was able to find the exact optimal solution in very short computational times.
We consider the problem of training generative models with deep neural networks as generators, i.e. to map latent codes to data points. Whereas the dominant paradigm combines simple priors over codes with complex deterministic models, we propose instead to use more flexible code distributions. These distributions are estimated non-parametrically by reversing the generator map during training. The benefits include: more powerful generative models, better modeling of latent structure and explicit control of the degree of generalization.
This paper is concerned with risk-sensitive performance analysis for linear quantum stochastic systems interacting with external bosonic fields. We consider a cost functional in the form of the exponential moment of the integral of a quadratic polynomial of the system variables over a bounded time interval. An integro-differential equation is obtained for the time evolution of this quadratic-exponential functional, which is compared with the original quantum risk-sensitive performance criterion employed previously for measurement-based quantum control and filtering problems. Using multi-point Gaussian quantum states for the past history of the system variables and their first four moments, we discuss a quartic approximation of the cost functional and its infinite-horizon asymptotic behaviour. The computation of the asymptotic growth rate of this approximation is reduced to solving two algebraic Lyapunov equations. We also outline further approximations of the cost functional, based on higher-order cumulants and their growth rates, together with large deviations estimates. For comparison, an auxiliary classical Gaussian Markov diffusion process is considered in a complex Euclidean space which reproduces the quantum system variables at the level of covariances but has different higher-order moments relevant to the risk-sensitive criteria. The results of the paper are also demonstrated by a numerical example and may find applications to coherent quantum risk-sensitive control problems, where the plant and controller form a fully quantum closed-loop system, and other settings with nonquadratic cost functionals.
Motivated by the interest of observing the growth of cancer cells among normal living cells and exploring how galaxies and stars are truly formed, the objective of this paper is to introduce a rigorous and effective method for counting point-masses, determining their spatial locations, and computing their attributes. Based on computation of Hermite moments that are Fourier-invariant, our approach facilitates the processing of both spatial and Fourier data in any dimension.
We study the multiview moduli problems that arise in computer vision. We show that these moduli spaces are always smooth and irreducible, in both the calibrated and uncalibrated cases, for any number of views. We also show that these moduli spaces always embed in (diagram) Hilbert schemes, and that these embeddings are open immersions for more than four views. Our approach also yields a natural smooth cover of the classical variety of essential matrices that seems not to have appeared in the literature to date.
Compressive sensing is a powerful technique for recovering sparse solutions of underdetermined linear systems, which is often encountered in uncertainty quantification analysis of expensive and high-dimensional physical models. We perform numerical investigations employing several compressive sensing solvers that target the unconstrained LASSO formulation, with a focus on linear systems that arise in the construction of polynomial chaos expansions. With core solvers of l1_ls, SpaRSA, CGIST, FPC_AS, and ADMM, we develop techniques to mitigate overfitting through an automated selection of regularization constant based on cross-validation, and a heuristic strategy to guide the stop-sampling decision. Practical recommendations on parameter settings for these techniques are provided and discussed. The overall method is applied to a series of numerical examples of increasing complexity, including large eddy simulations of supersonic turbulent jet-in-crossflow involving a 24-dimensional input. Through empirical phase-transition diagrams and convergence plots, we illustrate sparse recovery performance under structures induced by polynomial chaos, accuracy and computational tradeoffs between polynomial bases of different degrees, and practicability of conducting compressive sensing for a realistic, high-dimensional physical application. Across test cases studied in this paper, we find ADMM to have demonstrated empirical advantages through consistent lower errors and faster computational times.